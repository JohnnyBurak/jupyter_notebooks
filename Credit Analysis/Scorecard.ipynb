{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Учебный пример - скоркарта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим на уже знакомых нам данных скоркарту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подключаем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# подгружаем все нужные пакеты\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# игнорируем warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "%matplotlib inline\n",
    "\n",
    "# настройка внешнего вида графиков в seaborn\n",
    "sns.set_context(\n",
    "    \"notebook\", \n",
    "    font_scale = 1.5,       \n",
    "    rc = { \n",
    "        \"figure.figsize\" : (30, 30), \n",
    "        \"axes.titlesize\" : 18 \n",
    "    }\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import re\n",
    "import traceback\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('auto_app.csv', delimiter=\";\",decimal=\".\", encoding=\"windows-1251\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очистка и преобразование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем все преобразование как и в прошлый раз. Работа с пустыми значениями, векторизация, кодирование средним."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iin = df['IIN']\n",
    "df = df.drop(('IIN'), axis=1)\n",
    "df = df.drop(df.columns[[0,1]], axis=1)\n",
    "df = df.drop(('creditNumber'), axis=1)\n",
    "df = df.drop(('Вид страхования'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_columns = [c for c in df.columns if df[c].dtype.name == 'object']\n",
    "numerical_columns   = [c for c in df.columns if df[c].dtype.name != 'object']\n",
    "print (categorical_columns)\n",
    "print (numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(df[categorical_columns].count(axis=0)/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ll = [c for c in categorical_columns if (df[c].count()/df.shape[0])*100 <70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop((ll), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_describe = df.describe(include=[object])\n",
    "for c in ['Пол','Гражданство','Резиденство','Образование','Семейное положение',\\\n",
    "          'Адрес фактического проживания совпадает с адресом регистрации?','Отношение к месту проживания', \\\n",
    "          'Филиал', 'СПФ','Канал продаж'\n",
    "         ]:\n",
    "    df[c] = df[c].fillna(data_describe[c]['top'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop((['Должность клиента','Вид деятельности компании/организации']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_describe = df.describe(include=[object])\n",
    "for c in ['Цель кредитования','Условия кредитования','Наименование атосалона',\\\n",
    "          'Схема финансирования','Категория клиента',\\\n",
    "          'Статус занятости','Имеется работа по совместительству?',\\\n",
    "          'Являетесь ли вы лицом, освобожденным от уплаты обязательных пенсионных взносов в НПФ'\n",
    "         ]:\n",
    "    df[c] = df[c].fillna(data_describe[c]['top'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in ['Кредитная история в БВУ (автомат)']:\n",
    "    df[c] = df[c].fillna('Отсутствует')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df['Агент'].notnull(), 'Агент_new'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df['Агент'].isnull(), 'Агент_new'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop((['Агент']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_columns = [c for c in df.columns if df[c].dtype.name == 'object']\n",
    "numerical_columns   = [c for c in df.columns if df[c].dtype.name != 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_columns    = [c for c in categorical_columns if data_describe[c]['unique'] == 2]\n",
    "nonbinary_columns = [c for c in categorical_columns if data_describe[c]['unique'] > 2]\n",
    "print (binary_columns, nonbinary_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in binary_columns[0:]:\n",
    "    top = data_describe[c]['top']\n",
    "    top_items = df[c] == top\n",
    "    df.loc[top_items, c] = 1\n",
    "    df.loc[np.logical_not(top_items), c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in nonbinary_columns[0:]:\n",
    "    df[c+'_mean'] = df.groupby([c])['target'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop((nonbinary_columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filling(df):\n",
    "    cat_vars = df.select_dtypes(include=[object]).columns\n",
    "    num_vars = df.select_dtypes(include=[np.number]).columns\n",
    "    df[cat_vars] = df[cat_vars].fillna('_MISSING_')\n",
    "    df[num_vars] = df[num_vars].fillna(np.nan)\n",
    "    return df\n",
    "\n",
    "def replace_not_frequent(df, cols, perc_min=5, value_to_replace = \"0\"):\n",
    "        else_df = pd.DataFrame(columns=['var', 'list'])\n",
    "        for i in cols:\n",
    "            if i != 'date_requested' and i != 'credit_id':\n",
    "                t = df[i].value_counts(normalize=True)\n",
    "                q = list(t[t.values < perc_min/100].index)\n",
    "                if q:\n",
    "                    else_df = else_df.append(pd.DataFrame([[i, q]], columns=['var', 'list']))\n",
    "                df.loc[df[i].value_counts(normalize=True)[df[i]].values < perc_min/100, i] =value_to_replace\n",
    "        else_df = else_df.set_index('var')\n",
    "        return df, else_df\n",
    "\n",
    "cat_vars = df.select_dtypes(include=[object]).columns\n",
    "df = filling(df)\n",
    "\n",
    "df, else_df = replace_not_frequent(df, cat_vars)\n",
    "\n",
    "df.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.33, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Монотонный WOE binning признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь начинается самой важный этап в скоринге для регресии – необходимо написать WOE-binning для числовых и категориальных переменных.\n",
    "\n",
    "Монотонный WOE binning признаков:\n",
    "- Упрощает интерпретацию\n",
    "- Обрабатывает отсутствующие значения\n",
    "- Выявляет сложные нелинейные связи\n",
    "- Преобразование основано на логарифмическом распределении \n",
    "- Упрощает обработку выбросов\n",
    "- Нет необходимости в dummy variables\n",
    "- Можно устанавливать монотонную зависимость (либо увеличение, либо уменьшение) между независимой и зависимой переменной\n",
    "\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*6Aw782wiyiFtzvK7EOY8CA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_bin = 20\n",
    "force_bin = 3\n",
    "\n",
    "# define a binning function\n",
    "def mono_bin(Y, X, n = max_bin):\n",
    "    \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
    "    r = 0\n",
    "    while np.abs(r) < 1:\n",
    "        try:\n",
    "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
    "            d2 = d1.groupby('Bucket', as_index=True)\n",
    "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "            n = n - 1 \n",
    "        except Exception as e:\n",
    "            n = n - 1\n",
    "\n",
    "    if len(d2) == 1:\n",
    "        n = force_bin         \n",
    "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
    "        if len(np.unique(bins)) == 2:\n",
    "            bins = np.insert(bins, 0, 1)\n",
    "            bins[1] = bins[1]-(bins[1]/2)\n",
    "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
    "        d2 = d1.groupby('Bucket', as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"MIN_VALUE\"] = d2.min().X\n",
    "    d3[\"MAX_VALUE\"] = d2.max().X\n",
    "    d3[\"COUNT\"] = d2.count().Y\n",
    "    d3[\"EVENT\"] = d2.sum().Y\n",
    "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
    "    d3=d3.reset_index(drop=True)\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def char_bin(Y, X):\n",
    "        \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
    "    df2 = notmiss.groupby('X',as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"COUNT\"] = df2.count().Y\n",
    "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
    "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "    d3[\"EVENT\"] = df2.sum().Y\n",
    "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    d3 = d3.reset_index(drop=True)\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def data_vars(df1, target):\n",
    "    \n",
    "    stack = traceback.extract_stack()\n",
    "    filename, lineno, function_name, code = stack[-2]\n",
    "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
    "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
    "    \n",
    "    x = df1.dtypes.index\n",
    "    count = -1\n",
    "    \n",
    "    for i in x:\n",
    "        if i.upper() not in (final.upper()):\n",
    "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
    "                conv = mono_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i\n",
    "                count = count + 1\n",
    "            else:\n",
    "                conv = char_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i            \n",
    "                count = count + 1\n",
    "                \n",
    "            if count == 0:\n",
    "                iv_df = conv\n",
    "            else:\n",
    "                iv_df = iv_df.append(conv,ignore_index=True)\n",
    "    \n",
    "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
    "    iv = iv.reset_index()\n",
    "    return(iv_df,iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_iv, IV = data_vars(df,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Value (IV)** измеряет предсказательную силу признаков.Считается для каждого признака.\n",
    "![](https://cdn-images-1.medium.com/max/800/1*9Gi0fGyTpxfwM2TpV4GZQQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения Information Value (IV) для определения cutoff:\n",
    "- <0.02 Бесполезно для предсказания\n",
    "- 0.02 – 0.1 Слабая\n",
    "- 0.1 – 0.3 Средняя\n",
    "- 0.3 – 0.5 Хорошая\n",
    "- 0.5+ Слишком хорошо, что бы быть правдой\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IV[IV['VAR_NAME']=='Пол']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:40,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IV.sort_values('IV', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно посмотреть на графиках, как переменные разбились по группам и проверить монотонность или возрастает или убывает. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_bin(ev, for_excel=False):\n",
    "    ind = np.arange(len(ev.index)) \n",
    "    width = 0.35\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 7))\n",
    "    ax2 = ax1.twinx()\n",
    "    p1 = ax1.bar(ind, ev['NONEVENT'], width, color=(24/254, 192/254, 196/254))\n",
    "    p2 = ax1.bar(ind, ev['EVENT'], width, bottom=ev['NONEVENT'], color=(246/254, 115/254, 109/254))\n",
    "\n",
    "    ax1.set_ylabel('Event Distribution', fontsize=15)\n",
    "    ax2.set_ylabel('WOE', fontsize=15)\n",
    "\n",
    "    plt.title(list(ev.VAR_NAME)[0], fontsize=20) \n",
    "    ax2.plot(ind, ev['WOE'], marker='o', color='blue')\n",
    "    # Legend\n",
    "    plt.legend((p2[0], p1[0]), ('bad', 'good'), loc='best', fontsize=10)\n",
    "\n",
    "    #Set xticklabels\n",
    "    q = list()\n",
    "    for i in range(len(ev)):\n",
    "        try:\n",
    "            mn = str(round(ev.MIN_VALUE[i], 2))\n",
    "            mx = str(round(ev.MAX_VALUE[i], 2))\n",
    "        except:\n",
    "            mn = str((ev.MIN_VALUE[i]))\n",
    "            mx = str((ev.MAX_VALUE[i]))\n",
    "        q.append(mn + '-' + mx)\n",
    "\n",
    "    plt.xticks(ind, q, rotation='vertical')\n",
    "    for tick in ax1.get_xticklabels():\n",
    "        tick.set_rotation(60)\n",
    "    #plt.savefig('{}.png'.format(ev.VAR_NAME[0]), dpi=500, bbox_inches = 'tight')\n",
    "    plt.show() \n",
    "\n",
    "def plot_all_bins(iv_df):\n",
    "    for i in [x.replace('WOE_','') for x in X_train.columns]:\n",
    "        ev = iv_df[iv_df.VAR_NAME==i]\n",
    "        ev.reset_index(inplace=True)\n",
    "        plot_bin(ev)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_bins(final_iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все графики монотонно возрастают или убывают. Ручной биниинг не требуется. Ручной бининг нужен для объединения категориий с близкими значениями WOE в одну категорию так, чтобы максимизировать разницу между группами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def adjust_binning(df, bins_dict):\n",
    "    for i in range(len(bins_dict)):\n",
    "        key = list(bins_dict.keys())[i]\n",
    "        if type(list(bins_dict.values())[i])==dict:\n",
    "            df[key] = df[key].map(list(bins_dict.values())[i])\n",
    "        else:\n",
    "            #Categories labels\n",
    "            categories = list()\n",
    "            for j in range(len(list(bins_dict.values())[i])):\n",
    "                if j == 0:\n",
    "                    categories.append('<'+ str(list(bins_dict.values())[i][j]))\n",
    "                    try:                        \n",
    "                        categories.append('(' + str(list(bins_dict.values())[i][j]) +'; '+ str(list(bins_dict.values())[i][j+1]) + ']')\n",
    "                    except:                       \n",
    "                        categories.append('(' + str(list(bins_dict.values())[i][j]))\n",
    "                elif j==len(list(bins_dict.values())[i])-1:\n",
    "                    categories.append(str(list(bins_dict.values())[i][j]) +'>')\n",
    "                else:\n",
    "                    categories.append('(' + str(list(bins_dict.values())[i][j]) +'; '+ str(list(bins_dict.values())[i][j+1]) + ']')\n",
    "            \n",
    "            values = [df[key].min()] + list(bins_dict.values())[i]  + [df[key].max()]        \n",
    "            df[key + '_bins'] = pd.cut(df[key], values, include_lowest=True, labels=categories).astype(object).fillna('_MISSING_').astype(str)\n",
    "            df[key] = df[key + '_bins']#.map(df.groupby(key + '_bins')[key].agg('median'))\n",
    "            df.drop([key + '_bins'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "#bins_dict = {    \n",
    "#   'equi_delinquencyDays': [ 200,400,600]\n",
    "#    'loan_purpose': {'medicine':'1_group',\n",
    " #                   'repair':'1_group',\n",
    " #                   'helpFriend':'2_group'}\n",
    "#}\n",
    " \n",
    "#df = adjust_binning(df, bins_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После осуществлялась проверка на корреляцию. Из двух коррелирующих переменных нужно удалить ту, у которой IV меньше. Кат офф по удалению был взят 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def delete_correlated_features(df, cut_off=0.75, exclude = []):\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    \n",
    "    # Plotting All correlations\n",
    "    f, ax = plt.subplots(figsize=(15, 10))\n",
    "    plt.title('All correlations', fontsize=20)\n",
    "    sns.heatmap(X_train.corr(), annot=True)\n",
    "    \n",
    "    # Plotting highly correlated\n",
    "    try:\n",
    "        f, ax = plt.subplots(figsize=(15, 10))\n",
    "        plt.title('High correlated', fontsize=20)\n",
    "        sns.heatmap(corr_matrix[(corr_matrix>cut_off) & (corr_matrix!=1)].dropna(axis=0, how='all').dropna(axis=1, how='all'), annot=True, linewidths=.5)\n",
    "    except:\n",
    "        print ('No highly correlated features found')\n",
    "        \n",
    "    # Find index of feature columns with correlation greater than cut_off\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > cut_off)]\n",
    "    to_drop = [column for column in to_drop if column not in exclude]\n",
    "    print ('Dropped columns:', to_drop, '\\n')\n",
    "    df2 = df.drop(to_drop, axis=1)\n",
    "    print ('Features left after correlation check: {}'.format(len(df.columns)-len(to_drop)), '\\n')    \n",
    "   \n",
    "    print ('Not dropped columns:', list(df2.columns), '\\n')\n",
    "    \n",
    "    # Plotting final correlations\n",
    "    f, ax = plt.subplots(figsize=(15, 10))\n",
    "    plt.title('Final correlations', fontsize=20)\n",
    "    sns.heatmap(df2.corr(), annot=True)\n",
    "    plt.show()\n",
    "    \n",
    "    return df2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = delete_correlated_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо отбора по IV мы добавили рекурсивный поиск оптимального количества переменных методом RFE из sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def RFE_feature_selection(clf_lr, X, y):\n",
    "    rfecv = RFECV(estimator=clf_lr, step=1, cv=StratifiedKFold(5), verbose=0, scoring='roc_auc')\n",
    "    rfecv.fit(X, y)\n",
    "\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    f, ax = plt.subplots(figsize=(14, 9))\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "    mask = rfecv.get_support()\n",
    "    X = X.ix[:, mask]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее строилась регрессия и оценивались её метрики на кросс-валидации и тестовой выборке. Обычно все смотрят на коэффициент ROC AUC или Gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.33, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, accuracy_score, roc_auc_score, f1_score, log_loss, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_score(clf, X_test, y_test, feat_to_show=30, is_normalize=False, cut_off=0.5):\n",
    "    #cm = confusion_matrix(pd.Series(clf.predict_proba(X_test)[:,1]).apply(lambda x: 1 if x>cut_off else 0), y_test)\n",
    "    print ('ROC_AUC:  ', round(roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]), 3))\n",
    "    print ('Gini:     ', round(2*roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]) - 1, 3))\n",
    "    print ('F1_score: ', round(f1_score(y_test, clf.predict(X_test)), 3))\n",
    "    print ('Log_loss: ', round(log_loss(y_test, clf.predict(X_test)), 3))\n",
    "    \n",
    "    print ('\\n')\n",
    "    print ('Classification_report: \\n', classification_report(pd.Series(clf.predict_proba(X_test)[:,1]).apply(lambda x: 1 if x>cut_off else 0), y_test))\n",
    "    skplt.metrics.plot_confusion_matrix(y_test, pd.Series(clf.predict_proba(X_test)[:,1]).apply(lambda x: 1 if x>cut_off else 0), title=\"Confusion Matrix\",\n",
    "                    normalize=is_normalize,figsize=(8,8),text_fontsize='large')\n",
    "    display(eli5.show_weights(clf, top=20, feature_names = list(X_test.columns)))\n",
    "\n",
    "clf_lr = LogisticRegressionCV(random_state=1, cv=7)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "plot_score(clf_lr, X_test, y_test, cut_off=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " RFE_feature_selection(clf_lr,X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Как мы видим на графике – после 3 переменных качество не изменяется, а значит лишние можно удалить. Для регрессии более 15 переменных в скоринге считается плохим тоном, что в большинстве случаев исправляется с помощью RFE. Модель получилась слабой и нам рекомендуют исипользовать только три признака: Запрошенный срок кредита, Стаж работы на последнем месте работы, Прочие расходы, в месяц. Даи веса все нулевые. Нужно подтюнить модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление скорбаллов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({\"Feature\":X_train.columns,\"Coefficients\":np.transpose(clf_lr.coef_[0] )}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на коээфициенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefficients.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_lr.intercept_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель на столько ужансная, что у нас везде коэффиценты нули. Нормально преоброзовать в скорбаллы не получится. А тюнить модель для учебного примера лень=). Поэтому для примера преобразования мы позьмем для признака Возраст заявителя коэффициент равный 0,2\n",
    "\n",
    "Скорбалл считается так:\n",
    "Score = (β×WoE+ α/n)×Factor + Offset/n\n",
    "Where:\n",
    "- β — logistic regression coefficient for characteristics that contains the given attribute\n",
    "- α — logistic regression intercept \n",
    "- WoE — Weight of Evidence value for the given attribute\n",
    "- n — number of characteristics included in the model\n",
    "- Factor, Offset — scaling parameter\n",
    "\n",
    "The first four parameters have already been calculated is the previous part. The following formulas are used for calculating factor and offset.\n",
    "- Factor = pdo/Ln(2)\n",
    "- Offset = Score — (Factor × ln(Odds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IV[IV['VAR_NAME']=='Возраст заявителя']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefficients[coefficients['Feature']=='Возраст заявителя'].Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta = coefficients[coefficients['Feature']=='Возраст заявителя'].Coefficients.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "factor = 40/math.log(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offset = 600 -57.7 * math.log(72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_iv[final_iv['VAR_NAME']=='Возраст заявителя'].WOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "o = offset/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for kk in final_iv[final_iv['VAR_NAME']=='Возраст заявителя'].WOE:\n",
    "    #print(kk)\n",
    "    ll = round(beta*kk+alpha/n,3)\n",
    "    tt=ll*factor\n",
    "    score = tt + o\n",
    "    print(round(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_iv[final_iv['VAR_NAME']=='Возраст заявителя']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом получаем следующие скорбаллы:\n",
    "- Возраст заявителя от - 5 до 3 :10 баллов\n",
    "- Возраст заявителя от  34 до 43 :7 баллов\n",
    "- Возраст заявителя от  44 до 905 :6 баллов\n",
    "\n",
    "минус 5 и 905 - это выбросы, надо было от них избавиться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт в Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('Score.xlsx', engine='xlsxwriter')\n",
    "\n",
    "workbook  = writer.book\n",
    "worksheet = workbook.add_worksheet('Sample information')\n",
    "bold = workbook.add_format({'bold': True})\n",
    "percent_fmt = workbook.add_format({'num_format': '0.00%'})\n",
    "\n",
    "worksheet.set_column('A:A', 20)\n",
    "worksheet.set_column('B:B', 15)\n",
    "worksheet.set_column('C:C', 10)\n",
    "\n",
    "# Sample\n",
    "worksheet.write('A2', 'Sample conditions', bold)\n",
    "worksheet.write('A3', 1)\n",
    "worksheet.write('A4', 2)\n",
    "worksheet.write('A5', 3)\n",
    "worksheet.write('A6', 4)\n",
    "\n",
    "# Model\n",
    "worksheet.write('A8', 'Model development', bold)\n",
    "\n",
    "worksheet.write('A9', 1)\n",
    "#labels\n",
    "worksheet.write('C8', 'Bads')\n",
    "worksheet.write('D8', 'Goods')\n",
    "worksheet.write('B9', 'Train')\n",
    "worksheet.write('B10', 'Valid')\n",
    "worksheet.write('B11', 'Total')\n",
    "\n",
    "# goods and bads\n",
    "worksheet.write('C9', y_train.value_counts()[1])\n",
    "worksheet.write('C10', y_test.value_counts()[1])\n",
    "worksheet.write('D9', y_train.value_counts()[0])\n",
    "worksheet.write('D10', y_test.value_counts()[0])\n",
    "worksheet.write('C11', y.value_counts()[1])\n",
    "worksheet.write('D11', y.value_counts()[0])\n",
    "\n",
    "# NPL\n",
    "worksheet.write('A13', 2)\n",
    "worksheet.write('B13', 'NPL')\n",
    "worksheet.write('C13', (y.value_counts()[1]/(y.value_counts()[1]+y.value_counts()[0])), percent_fmt)\n",
    "\n",
    "worksheet.write('A16', 3)\n",
    "worksheet.write('C15', 'Gini')\n",
    "worksheet.write('B16', 'Train')\n",
    "worksheet.write('B17', 'Valid')\n",
    "worksheet.write('B18', 'CV Scores')\n",
    "worksheet.write('C18', str([round(sc, 2) for sc in scores]))\n",
    "\n",
    "worksheet.write('C16', round(2*roc_auc_score(y_train, clf_lr.predict_proba(X_train)[:,1]) - 1, 3))\n",
    "worksheet.write('C17', round(2*roc_auc_score(y_test, clf_lr.predict_proba(X_test)[:,1]) - 1, 3))\n",
    "\n",
    "# Regreesion coefs\n",
    "feat.to_excel(writer, sheet_name='Regression coefficients', index=False)\n",
    "worksheet2 = writer.sheets['Regression coefficients']\n",
    "worksheet2.set_column('A:A', 15)\n",
    "worksheet2.set_column('B:B', 50)\n",
    "\n",
    "#WOE\n",
    "\n",
    "ivs[['VAR_NAME', 'Variable range', 'WOE', 'COUNT', 'WOE_group']].to_excel(writer, sheet_name='WOE', index=False)\n",
    "worksheet3 = writer.sheets['WOE']\n",
    "worksheet3.set_column('A:A', 50)\n",
    "worksheet3.set_column('B:B', 60)\n",
    "worksheet3.set_column('C:C', 30)\n",
    "worksheet3.set_column('D:D', 20)\n",
    "worksheet3.set_column('E:E', 12)\n",
    "for num, i in enumerate([x.replace('WOE_','') for x in X_train.columns]):\n",
    "        ev = iv_df[iv_df.VAR_NAME==i]\n",
    "        ev.reset_index(inplace=True)\n",
    "        worksheet3.insert_image('G{}'.format(num*34+1), '{}.png'.format(i))\n",
    "\n",
    "df3.to_excel(writer, sheet_name='Data', index=False)\n",
    "\n",
    "table.to_excel(writer, sheet_name='Scores by buckets', header = True, index = True)\n",
    "worksheet4 = writer.sheets['Scores by buckets']\n",
    "worksheet4.set_column('A:A', 20)\n",
    "worksheet4.insert_image('J1', 'score_distribution.png')\n",
    "Ginis.to_excel(writer, sheet_name='Gini distribution', header = True, index = True)\n",
    "worksheet5 = writer.sheets['Gini distribution']\n",
    "worksheet5.insert_image('E1', 'gini_stability.png')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
