{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get torchvision utils for mask-rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../input/maskrcnn-utils/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install latest torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/45/0f2f3062c92d9cf1d5d7eabd3cae88cea9affbd2b17fb1c043627838cb0a/torchvision-0.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 2.6MB 11.4MB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision) (5.1.0)\r\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from torchvision) (1.12.0)\r\n",
      "Collecting torch>=1.1.0 (from torchvision)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 676.9MB 54kB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision) (1.16.3)\r\n",
      "Installing collected packages: torch, torchvision\r\n",
      "  Found existing installation: torch 1.0.1.post2\r\n",
      "    Uninstalling torch-1.0.1.post2:\r\n",
      "      Successfully uninstalled torch-1.0.1.post2\r\n",
      "  Found existing installation: torchvision 0.2.2\r\n",
      "    Uninstalling torchvision-0.2.2:\r\n",
      "      Successfully uninstalled torchvision-0.2.2\r\n",
      "Successfully installed torch-1.1.0 torchvision-0.3.0\r\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import everything useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "from model import get_instance_segmentation_model\n",
    "import torch\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from model import get_instance_segmentation_model\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import itertools\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset class for getting batches of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated: [start0] [length0] [start1] [length1]... in 1d array\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array according to the shape, 1 - mask, 0 - background\n",
    "    '''\n",
    "    shape = (shape[1], shape[0])\n",
    "    s = mask_rle.split()\n",
    "    # gets starts & lengths 1d arrays\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]\n",
    "    starts -= 1\n",
    "    # gets ends 1d array\n",
    "    ends = starts + lengths\n",
    "    # creates blank mask image 1d array\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    # sets mark pixles\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    # reshape as a 2d mask image\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "\n",
    "\n",
    "class FashionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, df_path, height, width, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.image_dir = image_dir\n",
    "        self.df = pd.read_csv(df_path, nrows=10000)\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.image_info = collections.defaultdict(dict)\n",
    "        self.df['CategoryId'] = self.df.ClassId.apply(lambda x: str(x).split(\"_\")[0])\n",
    "        temp_df = self.df.groupby('ImageId')['EncodedPixels', 'CategoryId'].agg(lambda x: list(x)).reset_index()\n",
    "        size_df = self.df.groupby('ImageId')['Height', 'Width'].mean().reset_index()\n",
    "        temp_df = temp_df.merge(size_df, on='ImageId', how='left')\n",
    "        for index, row in tqdm(temp_df.iterrows(), total=len(temp_df)):\n",
    "            image_id = row['ImageId']\n",
    "            image_path = os.path.join(self.image_dir, image_id)\n",
    "            self.image_info[index][\"image_id\"] = image_id\n",
    "            self.image_info[index][\"image_path\"] = image_path\n",
    "            self.image_info[index][\"width\"] = self.width\n",
    "            self.image_info[index][\"height\"] = self.height\n",
    "            self.image_info[index][\"labels\"] = row[\"CategoryId\"]\n",
    "            self.image_info[index][\"orig_height\"] = row[\"Height\"]\n",
    "            self.image_info[index][\"orig_width\"] = row[\"Width\"]\n",
    "            self.image_info[index][\"annotations\"] = row[\"EncodedPixels\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = self.image_info[idx][\"image_path\"]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "        mask = np.zeros((len(info['annotations']), self.width, self.height), dtype=np.uint8)\n",
    "        labels = []\n",
    "        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n",
    "            sub_mask = rle_decode(annotation, (info['orig_height'], info['orig_width']))\n",
    "            sub_mask = Image.fromarray(sub_mask)\n",
    "            sub_mask = sub_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n",
    "            mask[m, :, :] = sub_mask\n",
    "            labels.append(int(label) + 1)\n",
    "\n",
    "        num_objs = len(labels)\n",
    "        boxes = []\n",
    "        new_labels = []\n",
    "        new_masks = []\n",
    "\n",
    "        for i in range(num_objs):\n",
    "            try:\n",
    "                pos = np.where(mask[i, :, :])\n",
    "                xmin = np.min(pos[1])\n",
    "                xmax = np.max(pos[1])\n",
    "                ymin = np.min(pos[0])\n",
    "                ymax = np.max(pos[0])\n",
    "                if abs(xmax - xmin) >= 20 and abs(ymax - ymin) >= 20:\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    new_labels.append(labels[i])\n",
    "                    new_masks.append(mask[i, :, :])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        if len(new_labels) == 0:\n",
    "            boxes.append([0, 0, 20, 20])\n",
    "            new_labels.append(0)\n",
    "            new_masks.append(mask[0, :, :])\n",
    "\n",
    "        nmx = np.zeros((len(new_masks), self.width, self.height), dtype=np.uint8)\n",
    "        for i, n in enumerate(new_masks):\n",
    "            nmx[i, :, :] = n\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(new_labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(nmx, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1355/1355 [00:00<00:00, 5706.84it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /tmp/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
      "100%|██████████| 178090079/178090079 [00:07<00:00, 24651012.29it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'MetricLogger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-76c77220adf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/input/maskrcnn-utils/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmetric_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetricLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_meter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSmoothedValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{value:.6f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Epoch: [{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'MetricLogger'"
     ]
    }
   ],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "num_classes = 46 + 1\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "dataset_train = FashionDataset(\"../input/imaterialist-fashion-2019-FGVC6/train/\",\n",
    "                               \"../input/imaterialist-fashion-2019-FGVC6/train.csv\",\n",
    "                               256,\n",
    "                               256,\n",
    "                               transforms=get_transform(train=True))\n",
    "\n",
    "\n",
    "model_ft = get_instance_segmentation_model(num_classes)\n",
    "model_ft.to(device)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=4, shuffle=True, num_workers=8,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=5,\n",
    "                                               gamma=0.1)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model_ft, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "torch.save(model_ft.state_dict(), \"model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Columns not found: 'Width', 'Height'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-93a02ba6a189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m dataset_test = FashionDataset(\"../input/imaterialist-fashion-2019-FGVC6/test/\", \n\u001b[1;32m     26\u001b[0m                               \u001b[0;34m\"../input/imaterialist-fashion-2019-FGVC6/sample_submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                               transforms=None)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0msample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/imaterialist-fashion-2019-FGVC6/sample_submission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4557e2f6ca8b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_dir, df_path, height, width, transforms)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CategoryId'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassId\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ImageId'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EncodedPixels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CategoryId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0msize_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ImageId'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Height'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ImageId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mbad_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 raise KeyError(\"Columns not found: {missing}\"\n\u001b[0;32m--> 257\u001b[0;31m                                .format(missing=str(bad_keys)[1:-1]))\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Columns not found: 'Width', 'Height'\""
     ]
    }
   ],
   "source": [
    "def refine_masks(masks, labels):\n",
    "   # Compute the areas of each mask\n",
    "   areas = np.sum(masks.reshape(-1, masks.shape[-1]), axis=0)\n",
    "   # Masks are ordered from smallest to largest\n",
    "   mask_index = np.argsort(areas)\n",
    "   # One reference mask is created to be incrementally populated\n",
    "   union_mask = {k:np.zeros(masks.shape[:-1], dtype=bool) for k in np.unique(labels)}\n",
    "   # Iterate from the smallest, so smallest ones are preserved\n",
    "   for m in mask_index:\n",
    "       label = labels[m]\n",
    "       masks[:, :, m] = np.logical_and(masks[:, :, m], np.logical_not(union_mask[label]))\n",
    "       union_mask[label] = np.logical_or(masks[:, :, m], union_mask[label])\n",
    "   # Reorder masks\n",
    "   refined = list()\n",
    "   for m in range(masks.shape[-1]):\n",
    "       mask = masks[:, :, m].ravel(order='F')\n",
    "       rle = to_rle(mask)\n",
    "       label = labels[m] - 1\n",
    "       refined.append([masks[:, :, m], rle, label])\n",
    "   return refined\n",
    "\n",
    "\n",
    "num_classes = 46 + 1\n",
    "\n",
    "dataset_test = FashionDataset(\"../input/imaterialist-fashion-2019-FGVC6/test/\", \n",
    "                              \"../input/imaterialist-fashion-2019-FGVC6/sample_submission.csv\", 512, 512,\n",
    "                              transforms=None)\n",
    "\n",
    "sample_df = pd.read_csv(\"../input/imaterialist-fashion-2019-FGVC6/sample_submission.csv\")\n",
    "\n",
    "\n",
    "model_ft = get_instance_segmentation_model(num_classes)\n",
    "model_ft.load_state_dict(torch.load(\"model.bin\"))\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_ft.eval()\n",
    "\n",
    "\n",
    "sub_list = []\n",
    "missing_count = 0\n",
    "submission = []\n",
    "ctr = 0\n",
    "\n",
    "tk0 = tqdm(range(3200))\n",
    "tt = transforms.ToTensor()\n",
    "for i in tk0:\n",
    "    img = dataset_test[i]\n",
    "    img = tt(img)\n",
    "    result = model_ft([img.to(device)])[0]\n",
    "    masks = np.zeros((512, 512, len(result[\"masks\"])))\n",
    "    for j, m in enumerate(result[\"masks\"]):\n",
    "        res = transforms.ToPILImage()(result[\"masks\"][j].permute(1, 2, 0).cpu().numpy())\n",
    "        res = np.asarray(res.resize((512, 512), resample=Image.BILINEAR))\n",
    "        masks[:, :, j] = (res[:, :] * 255. > 127).astype(np.uint8)\n",
    "\n",
    "    lbls = result['labels'].cpu().numpy()\n",
    "    scores = result['scores'].cpu().numpy()\n",
    "\n",
    "    best_idx = 0\n",
    "    for scr in scores:\n",
    "      if scr > 0.8:\n",
    "        best_idx += 1\n",
    "\n",
    "    if best_idx == 0:\n",
    "      sub_list.append([sample_df.loc[i, 'ImageId'], '1 1', 23])\n",
    "      missing_count += 1\n",
    "      continue\n",
    "\n",
    "    if masks.shape[-1] > 0:\n",
    "        #lll = mask_to_rle(masks[:, :, :4], scores[:4], lbls[:4])\n",
    "        masks = refine_masks(masks[:, :, :best_idx], lbls[:best_idx])\n",
    "        for m, rle, label in masks:\n",
    "            sub_list.append([sample_df.loc[i, 'ImageId'], ' '.join(list(map(str, list(rle)))), label])\n",
    "    else:\n",
    "        sub_list.append([sample_df.loc[i, 'ImageId'], '1 1', 23])\n",
    "        missing_count += 1\n",
    "\n",
    "submission_df = pd.DataFrame(sub_list, columns=sample_df.columns.values)\n",
    "print(\"Total image results: \", submission_df['ImageId'].nunique())\n",
    "print(\"Missing Images: \", missing_count)\n",
    "submission_df = submission_df[submission_df.EncodedPixels.notnull()]\n",
    "for row in range(len(submission_df)):\n",
    "   line = submission_df.iloc[row,:]\n",
    "   submission_df.iloc[row, 1] = line['EncodedPixels'].replace('.0','')\n",
    "submission_df.head()\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result\n",
    "\n",
    "To run the code properly, download the attached dataset and run it locally instead of kaggle kernels. :) \n",
    "\n",
    "The approach wont give you 0.17+ directly. It requires very small modifications to get that kind of score. I’ll leave those modifcations as an exercise to the reader.\n",
    "\n",
    "If you have any questions, feel free to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
