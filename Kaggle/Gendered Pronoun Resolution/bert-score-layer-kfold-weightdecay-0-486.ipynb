{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The basic idea is from my kernel (https://www.kaggle.com/chanhu/bert-score-layer-lb-0-475).\n",
    "In this kernel, I had changed several points below.\n",
    "* keras -> pytorch(this is my second kernel wrote in pytorch)\n",
    "* use pretrain Bert, EndpointSpanExtractor, and weight decay.\n",
    "(similar to Lee's work https://www.kaggle.com/ceshine/pytorch-bert-endpointspanextractor-kfold) \n",
    "* use kfold to get a robust score.(according to the comment from Matei Ionita, and huiqin. Thanks!)\n",
    "\n",
    "P.S: the best I can get is 0.486. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__notebook__.ipynb', 'best_model_1.hdf5', 'contextual_embeddings_gap_test.json', 'best_model_5.hdf5', 'best_model_3.hdf5', 'submission.csv', '__output__.json', 'uncased_L-12_H-768_A-12.zip', '__pycache__', 'contextual_embeddings_gap_validation.json', 'extract_features.py', '__results___files', 'custom.css', 'best_model_4.hdf5', 'uncased_L-12_H-768_A-12', 'contextual_embeddings_gap_train.json', 'tokenization.py', '__results__.html', 'train_dist_df.csv', 'best_model_2.hdf5', 'modeling.py', 'val_dist_df.csv', 'test_dist_df.csv']\n",
      "['gap-test.tsv', 'gap-validation.tsv', 'gap-development.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "print(os.listdir('../input/bert-score-layer-lb-0-475'))\n",
    "print(os.listdir('../input/gap-coreference'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "3535d049dab0510188e7cb3d7319b8ea245b9505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The conda.compat module is deprecated and will be removed in a future release.\r\n",
      "Collecting package metadata: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /opt/conda\r\n",
      "\r\n",
      "  removed specs:\r\n",
      "    - greenlet\r\n",
      "\r\n",
      "\r\n",
      "The following packages will be downloaded:\r\n",
      "\r\n",
      "    package                    |            build\r\n",
      "    ---------------------------|-----------------\r\n",
      "    conda-4.6.12               |           py36_1         2.1 MB\r\n",
      "    ------------------------------------------------------------\r\n",
      "                                           Total:         2.1 MB\r\n",
      "\r\n",
      "The following packages will be REMOVED:\r\n",
      "\r\n",
      "  gevent-1.3.0-py36h14c3975_0\r\n",
      "  greenlet-0.4.13-py36h14c3975_0\r\n",
      "\r\n",
      "The following packages will be UPDATED:\r\n",
      "\r\n",
      "  conda                                       4.6.11-py36_0 --> 4.6.12-py36_1\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Downloading and Extracting Packages\r\n",
      "conda-4.6.12         | 2.1 MB    | ##################################### | 100% \r\n",
      "Preparing transaction: \\ \b\bdone\r\n",
      "Verifying transaction: / \b\bdone\r\n",
      "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\r\n",
      "Collecting pytorch-pretrained-bert\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 3.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.31.1)\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.0.1.post2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.16.2)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2019.3.12)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.130)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.21.0)\r\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.130 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.130)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.130->boto3->pytorch-pretrained-bert) (2.6.0)\r\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.130->boto3->pytorch-pretrained-bert) (0.14)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.130->boto3->pytorch-pretrained-bert) (1.12.0)\r\n",
      "Installing collected packages: pytorch-pretrained-bert\r\n",
      "Successfully installed pytorch-pretrained-bert-0.6.1\r\n",
      "Collecting allennlp\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/c8/10342a6068a8d156a5947e03c95525d559e71ad62de0f2585ab922e14533/allennlp-0.8.3-py3-none-any.whl (5.6MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 5.6MB 6.6MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.9.130)\r\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.5.1)\r\n",
      "Collecting msgpack<0.6.0,>=0.5.6 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/4e/dcf124fd97e5f5611123d6ad9f40ffd6eb979d1efdc1049e28a795672fcd/msgpack-0.5.6-cp36-cp36m-manylinux1_x86_64.whl (315kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 30.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.20.3)\r\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.6)\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.1.post2)\r\n",
      "Collecting gevent>=1.3.6 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/ca/5b5962361ed832847b6b2f9a2d0452c8c2f29a93baef850bb8ad067c7bf9/gevent-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 5.5MB 6.6MB/s \r\n",
      "\u001b[?25hCollecting conllu==0.11 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.16.2)\r\n",
      "Collecting moto>=1.3.4 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/40/cec89fa5c13108eb1c8de435633f8b7639e0e43fcbcdc8ac52633efeeabe/moto-1.3.7-py2.py3-none-any.whl (552kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 29.6MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /opt/conda/lib/python3.6/site-packages (from allennlp) (4.31.1)\r\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.6/site-packages (from allennlp) (4.4.3)\r\n",
      "Requirement already satisfied: spacy<2.2,>=2.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.1.3)\r\n",
      "Collecting responses>=0.7 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\r\n",
      "Collecting overrides (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\r\n",
      "Collecting word2number>=1.1 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\r\n",
      "Collecting flaky (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\r\n",
      "Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/dc/3abd3971869a741d7acdba166d71d4f9366b6b53028dfd56f95de356af0f/jsonnet-0.12.1.tar.gz (240kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 245kB 30.4MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.0.3)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.9.0)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.2.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.1.0)\r\n",
      "Requirement already satisfied: flask>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2018.4)\r\n",
      "Collecting parsimonious>=0.8.0 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 24.4MB/s \r\n",
      "\u001b[?25hCollecting sqlparse>=0.2.4 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/53/900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/sqlparse-0.3.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.6.1)\r\n",
      "Collecting awscli>=1.11.91 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8f/4e52f8d24f59f968ae2f5194d6c7e2a8c3572b4a5d68dcc49836e6556680/awscli-1.16.143-py2.py3-none-any.whl (1.5MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 16.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpydoc>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.8.0)\r\n",
      "Requirement already satisfied: requests>=2.18 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.21.0)\r\n",
      "Collecting editdistance (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/67/2b1fe72bdd13ee9ec32b97959d7dfbfcd7c0548081d69aaf8493c1e695f9/editdistance-0.5.3-cp36-cp36m-manylinux1_x86_64.whl (178kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 35.5MB/s \r\n",
      "\u001b[?25hCollecting flask-cors>=3.0.7 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.23)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (0.2.0)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (0.9.4)\r\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.130 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (1.12.130)\r\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (1.5.3)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (1.12.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (39.1.0)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (18.1.0)\r\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (4.1.0)\r\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (0.6.0)\r\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\r\n",
      "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\" (from gevent>=1.3.6->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 24.2MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.10)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.6.0)\r\n",
      "Collecting jsondiff==1.1.1 (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/5f/13e28a2f9abeda2ffb3f44f2f809b01b52bc02cdb63816e05b8c9cbbdfc5/jsondiff-1.1.1.tar.gz\r\n",
      "Requirement already satisfied: mock in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.0.0)\r\n",
      "Collecting cryptography>=2.3.0 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 2.3MB 12.7MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: werkzeug in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (0.14.1)\r\n",
      "Requirement already satisfied: boto>=2.36.0 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.48.0)\r\n",
      "Collecting pyaml (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/33/1a/936074f3492156693fc9e471269fc5747fa3b7d9d7f8a33af054f6b24066/pyaml-19.4.1-py2.py3-none-any.whl\r\n",
      "Collecting aws-xray-sdk<0.96,>=0.93 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl (52kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 25.9MB/s \r\n",
      "\u001b[?25hCollecting docker>=2.5.1 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/68/c3afca1a5aa8d2997ec3b8ee822a4d752cf85907b321f07ea86888545152/docker-3.7.2-py2.py3-none-any.whl (134kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 35.8MB/s \r\n",
      "\u001b[?25hCollecting xmltodict (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\r\n",
      "Collecting python-jose<3.0.0 (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: html5lib in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp) (0.1.7)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (1.0.0)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (2.0.2)\r\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.2.4)\r\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (2.6.0)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.2.1)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.0.5)\r\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (2.0.1)\r\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.9.6)\r\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (7.0.4)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (2.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (0.24)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (7.0)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.3.12)\r\n",
      "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (3.12)\r\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.14)\r\n",
      "Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.3.9)\r\n",
      "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 23.2MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: sphinx>=1.2.3 in /opt/conda/lib/python3.6/site-packages (from numpydoc>=0.8.0->allennlp) (1.7.4)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (1.22)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (2019.3.9)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (2.6)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=2.7.3->moto>=1.3.4->allennlp) (1.0)\r\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.6/site-packages (from mock->moto>=1.3.4->allennlp) (5.1.3)\r\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (0.24.0)\r\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (1.11.5)\r\n",
      "Requirement already satisfied: jsonpickle in /opt/conda/lib/python3.6/site-packages (from aws-xray-sdk<0.96,>=0.93->moto>=1.3.4->allennlp) (0.9.6)\r\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.6/site-packages (from aws-xray-sdk<0.96,>=0.93->moto>=1.3.4->allennlp) (1.10.11)\r\n",
      "Collecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker>=2.5.1->moto>=1.3.4->allennlp) (0.56.0)\r\n",
      "Collecting ecdsa<1.0 (from python-jose<3.0.0->moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/a8/8aa68e70959e1287da9154e5164bb8bd5dd7025e41ae54e8d177b8d165c9/ecdsa-0.13.2-py2.py3-none-any.whl (59kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 26.8MB/s \r\n",
      "\u001b[?25hCollecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/fc/b09816d7b2d79d6454f75b40def94a89ed785d8d8d07840563f1084c6ecd/pycryptodome-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 9.7MB 4.2MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: future<1.0 in /opt/conda/lib/python3.6/site-packages (from python-jose<3.0.0->moto>=1.3.4->allennlp) (0.17.1)\r\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from html5lib->ftfy->allennlp) (0.5.1)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\r\n",
      "Requirement already satisfied: Pygments>=2.0 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.2.0)\r\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.2.1)\r\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.5.3)\r\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (0.7.10)\r\n",
      "Requirement already satisfied: imagesize in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.0.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (17.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3.0->moto>=1.3.4->allennlp) (2.18)\r\n",
      "Building wheels for collected packages: overrides, word2number, jsonnet, parsimonious, jsondiff\r\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\r\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\r\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/f0/47/51/a178b15274ed0db775a1ae9c799ce31e511609c3ab75a7dec5\r\n",
      "  Building wheel for parsimonious (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\r\n",
      "  Building wheel for jsondiff (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/68/08/07/69d839606fb7fdc778fa86476abc0a864693d45969a0c1936c\r\n",
      "Successfully built overrides word2number jsonnet parsimonious jsondiff\r\n",
      "\u001b[31mawscli 1.16.143 has requirement botocore==1.12.133, but you'll have botocore 1.12.130 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: msgpack, greenlet, gevent, conllu, responses, jsondiff, cryptography, pyaml, aws-xray-sdk, docker-pycreds, docker, xmltodict, ecdsa, pycryptodome, python-jose, moto, overrides, word2number, flaky, jsonnet, parsimonious, sqlparse, rsa, awscli, editdistance, flask-cors, allennlp\r\n",
      "  Found existing installation: msgpack 0.6.1\r\n",
      "    Uninstalling msgpack-0.6.1:\r\n",
      "      Successfully uninstalled msgpack-0.6.1\r\n",
      "  Found existing installation: cryptography 2.2.2\r\n",
      "    Uninstalling cryptography-2.2.2:\r\n",
      "      Successfully uninstalled cryptography-2.2.2\r\n",
      "  Found existing installation: rsa 4.0\r\n",
      "    Uninstalling rsa-4.0:\r\n",
      "      Successfully uninstalled rsa-4.0\r\n",
      "  Found existing installation: Flask-Cors 3.0.4\r\n",
      "    Uninstalling Flask-Cors-3.0.4:\r\n",
      "      Successfully uninstalled Flask-Cors-3.0.4\r\n",
      "Successfully installed allennlp-0.8.3 aws-xray-sdk-0.95 awscli-1.16.143 conllu-0.11 cryptography-2.6.1 docker-3.7.2 docker-pycreds-0.4.0 ecdsa-0.13.2 editdistance-0.5.3 flaky-3.5.3 flask-cors-3.0.7 gevent-1.4.0 greenlet-0.4.15 jsondiff-1.1.1 jsonnet-0.12.1 moto-1.3.7 msgpack-0.5.6 overrides-1.9 parsimonious-0.8.1 pyaml-19.4.1 pycryptodome-3.8.1 python-jose-2.0.2 responses-0.10.6 rsa-3.4.2 sqlparse-0.3.0 word2number-1.1 xmltodict-0.12.0\r\n"
     ]
    }
   ],
   "source": [
    "!conda remove -y greenlet\n",
    "!pip install pytorch-pretrained-bert\n",
    "!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 6032272.66B/s]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.span_extractors import EndpointSpanExtractor \n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentencizer = nlp.create_pipe('sentencizer')\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "803a2bcc22037e64732556cac31e2ce326c3696e"
   },
   "outputs": [],
   "source": [
    "def candidate_length(candidate):\n",
    "    #count the word length without space\n",
    "    count = 0\n",
    "    for i in range(len(candidate)):\n",
    "        if candidate[i] !=  \" \": count += 1\n",
    "    return count\n",
    "\n",
    "def count_char(text, offset):\n",
    "    count = 0\n",
    "    for pos in range(offset):\n",
    "        if text[pos] != \" \": count +=1\n",
    "    return count\n",
    "\n",
    "def count_token_length_special(token):\n",
    "    count = 0\n",
    "    special_token = [\"#\", \" \"]\n",
    "    for i in range(len(token)):\n",
    "        if token[i] not in special_token: \n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def find_word_index(tokenized_text, char_start, target):\n",
    "    tar_len = candidate_length(target)\n",
    "    char_count = 0\n",
    "    word_index = []\n",
    "    special_token = [\"[CLS]\", \"[SEP]\"]\n",
    "    for i in range(len(tokenized_text)):\n",
    "        token = tokenized_text[i]\n",
    "        if char_count in range(char_start, char_start+tar_len):\n",
    "            if token in special_token: # for the case like \"[SEP]. she\"\n",
    "                continue\n",
    "            word_index.append(i)\n",
    "        if token not in special_token:\n",
    "            token_length = count_token_length_special(token)\n",
    "            char_count += token_length\n",
    "    \n",
    "    if len(word_index) == 1:\n",
    "        return [word_index[0], word_index[0]] #the output will be start index of span, and end index of span\n",
    "    else:\n",
    "        return [word_index[0], word_index[-1]]\n",
    "\n",
    "def create_tokenizer_input(sents):\n",
    "    tokenizer_input = str()\n",
    "    for i, sent in enumerate(sents):\n",
    "        if i == 0:\n",
    "            tokenizer_input += \"[CLS] \"+sent.text+\" [SEP] \"\n",
    "        elif i == len(sents) - 1:\n",
    "            tokenizer_input += sent.text+\" [SEP]\"\n",
    "        else:\n",
    "            tokenizer_input += sent.text+\" [SEP] \"\n",
    "            \n",
    "    return  tokenizer_input\n",
    "\n",
    "def create_inputs(dataframe):\n",
    "    \n",
    "    idxs = dataframe.index\n",
    "    columns = ['indexed_token', 'offset']\n",
    "    features_df = pd.DataFrame(index=idxs, columns=columns)\n",
    "    max_len = 0\n",
    "    for i in tqdm(range(len(dataframe))):\n",
    "        text           = dataframe.loc[i, 'Text']\n",
    "        Pronoun_offset = dataframe.loc[i, 'Pronoun-offset']\n",
    "        A_offset       = dataframe.loc[i, \"A-offset\"]\n",
    "        B_offset       = dataframe.loc[i, \"B-offset\"]\n",
    "        Pronoun        = dataframe.loc[i, \"Pronoun\"]\n",
    "        A              = dataframe.loc[i, \"A\"]\n",
    "        B              = dataframe.loc[i, \"B\"]\n",
    "        doc            = nlp(text)\n",
    "        \n",
    "        sents = []\n",
    "        for sent in doc.sents: sents.append(sent)\n",
    "        token_input = create_tokenizer_input(sents)\n",
    "        token_input = token_input.replace(\"#\", \"*\") #Remove special symbols “#” from the original sentence\n",
    "        tokenized_text = tokenizer.tokenize(token_input) #the token text\n",
    "        if len(tokenized_text) > max_len: \n",
    "            max_len = len(tokenized_text) \n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) #token text to index\n",
    "        \n",
    "        A_char_start, B_char_start = count_char(text, A_offset), count_char(text, B_offset)\n",
    "        Pronoun_char_start         = count_char(text, Pronoun_offset)\n",
    "        \n",
    "        word_indexes = [] #\n",
    "        for char_start, target in zip([A_char_start, B_char_start, Pronoun_char_start], [A, B, Pronoun]):\n",
    "            word_indexes.append(find_word_index(tokenized_text, char_start, target))#\n",
    "        features_df.iloc[i] = [indexed_tokens, word_indexes]\n",
    "        \n",
    "    print('max length of sentence:', max_len)\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:08<00:00, 224.14it/s]\n",
      "  2%|▏         | 45/2000 [00:00<00:08, 224.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of sentence: 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:08<00:00, 224.46it/s]\n",
      "  9%|▉         | 41/454 [00:00<00:02, 204.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of sentence: 353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:02<00:00, 205.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of sentence: 237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('../input/gap-coreference/gap-test.tsv')\n",
    "test_df  = pd.read_table('../input/gap-coreference/gap-development.tsv')\n",
    "val_df   = pd.read_table('../input/gap-coreference/gap-validation.tsv')\n",
    "new_train_df = create_inputs(train_df)\n",
    "new_test_df  = create_inputs(test_df)\n",
    "new_val_df   = create_inputs(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "470f21a85ce86d3c8853776a20817d43bd4f6025"
   },
   "outputs": [],
   "source": [
    "def get_label(dataframe):\n",
    "    labels = []\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe.loc[i, 'A-coref']:\n",
    "            labels.append(0)\n",
    "        elif dataframe.loc[i, 'B-coref']:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(2)\n",
    "            \n",
    "    return labels\n",
    "\n",
    "new_train_df['label'] = get_label(train_df) # Add label columns\n",
    "new_val_df['label']   = get_label(val_df)\n",
    "new_df = pd.concat([new_train_df, new_val_df]) # combine train_df with val_df for the Kfold input \n",
    "new_df = new_df.reset_index(drop=True)\n",
    "new_df.to_csv('train.csv', index=False)\n",
    "new_test_df['label'] = get_label(test_df)\n",
    "new_test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_df\n",
    "del new_val_df\n",
    "del new_test_df\n",
    "del new_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "31405d984490fccf860fb68c5a9aceafc0d57cb8"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from ast import literal_eval\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        index_token = self.df.loc[idx, 'indexed_token']\n",
    "        index_token = literal_eval(index_token) # Change string to list\n",
    "        index_token = pad_sequences([index_token], maxlen=360, padding='post')[0] #pad \n",
    "        \n",
    "        offset = self.df.loc[idx, 'offset']\n",
    "        offset = literal_eval(offset)\n",
    "        offset = np.asarray(offset, dtype='int32')\n",
    "        label  = int(self.df.loc[idx, 'label'])\n",
    "        \n",
    "        distP_A = self.df.loc[idx, 'D_PA']\n",
    "        distP_B = self.df.loc[idx, 'D_PB']\n",
    "        \n",
    "        if self.transform:\n",
    "            index_token = self.transform(index_token)\n",
    "            offset = self.transform(offset)\n",
    "            label = self.transform(label)\n",
    "        \n",
    "        return (index_token, offset, distP_A, distP_B), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class score(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super(score, self).__init__()\n",
    "        self.score = torch.nn.Sequential(\n",
    "                     torch.nn.Linear(embed_dim, hidden_dim),\n",
    "                     torch.nn.ReLU(inplace=True),\n",
    "                     torch.nn.Dropout(0.6),\n",
    "                     torch.nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.score(x)\n",
    "    \n",
    "class mentionpair_score(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(mentionpair_score, self).__init__()\n",
    "        self.score = score(input_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, g1, g2, dist_embed):\n",
    "        \n",
    "        element_wise = g1 * g2\n",
    "        pair_score   = self.score(torch.cat((g1, g2, element_wise, dist_embed), dim=-1)) \n",
    "        \n",
    "        return pair_score\n",
    "\n",
    "class score_model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(score_model, self).__init__()\n",
    "        self.buckets        = [1, 2, 3, 4, 5, 8, 16, 32, 64] \n",
    "        self.bert           = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.embedding      = torch.nn.Embedding(len(self.buckets)+1, 20)\n",
    "        self.span_extractor = EndpointSpanExtractor(768, \"x,y,x*y\")\n",
    "        self.pair_score     = mentionpair_score(2304*3+20, 150)\n",
    "        \n",
    "    def forward(self, sent, offsets, distP_A, distP_B):\n",
    "        \n",
    "        bert_output, _   = self.bert(sent, output_all_encoded_layers=False) # (batch_size, max_len, 768)\n",
    "        #Distance Embeddings\n",
    "        distPA_embed     = self.embedding(distP_A)\n",
    "        distPB_embed     = self.embedding(distP_B)\n",
    "        \n",
    "        #Span Representation\n",
    "        span_repres     = self.span_extractor(bert_output, offsets) #(batch, 3, 2304)\n",
    "        span_repres     = torch.unbind(span_repres, dim=1) #[A: (bath, 2304), B: (bath, 2304), Pronoun:  (bath, 2304)]\n",
    "        span_norm = []\n",
    "        for i in range(len(span_repres)): \n",
    "            span_norm.append(F.normalize(span_repres[i], p=2, dim=1)) #normalizes the words embeddings\n",
    "    \n",
    "        ap_score = self.pair_score(span_norm[2], span_norm[0], distPA_embed)\n",
    "        bp_score = self.pair_score(span_norm[2], span_norm[1], distPB_embed)\n",
    "        nan_score = torch.zeros_like(ap_score)\n",
    "        output = torch.cat((ap_score, bp_score, nan_score), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Code from https://www.kaggle.com/ceshine/pytorch-bert-endpointspanextractor-kfold\n",
    "\n",
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, torch.nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "            \n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D_PA</th>\n",
       "      <th>D_PB</th>\n",
       "      <th>IN_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   D_PA  D_PB  IN_URL\n",
       "0     5     3       1\n",
       "1     6     5       0\n",
       "2     6     3       0\n",
       "3     7     0       1\n",
       "4     0     0       2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the distance features(distance between two word) are binned into the following buckets\n",
    "#[1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+]\n",
    "#D_PA is the distance of A and Pronoun\n",
    "#D_PB is the distance of B and Pronoun\n",
    "#You can check: https://aclweb.org/anthology/D17-1018\n",
    "\n",
    "train_dist = pd.read_csv('../input/bert-score-layer-lb-0-475/train_dist_df.csv')\n",
    "val_dist   = pd.read_csv('../input/bert-score-layer-lb-0-475/val_dist_df.csv')\n",
    "test_dist  = pd.read_csv('../input/bert-score-layer-lb-0-475/test_dist_df.csv')\n",
    "\n",
    "train_dist = pd.concat([train_dist, val_dist])\n",
    "train_dist = train_dist.reset_index(drop=True)\n",
    "train_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "n_split = 5\n",
    "\n",
    "train = pd.read_csv('../working/train.csv')\n",
    "test  = pd.read_csv('../working/test.csv')\n",
    "\n",
    "train = pd.concat([train, train_dist], axis=1)\n",
    "test  = pd.concat([test, test_dist], axis=1)\n",
    "train.head()\n",
    "Kfold = StratifiedKFold(n_splits=n_split, random_state=2019).split(train, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [00:08<00:00, 48645633.91B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "Epoch 1/30 \t loss=0.9220 \t val_loss=0.8036 \t time=35.35s\n",
      "Epoch 2/30 \t loss=0.7807 \t val_loss=0.7198 \t time=35.22s\n",
      "Epoch 3/30 \t loss=0.7147 \t val_loss=0.6806 \t time=35.27s\n",
      "Epoch 4/30 \t loss=0.6743 \t val_loss=0.6480 \t time=35.25s\n",
      "Epoch 5/30 \t loss=0.6369 \t val_loss=0.6241 \t time=35.18s\n",
      "Epoch 6/30 \t loss=0.6182 \t val_loss=0.6221 \t time=35.20s\n",
      "Epoch 7/30 \t loss=0.5918 \t val_loss=0.6084 \t time=35.27s\n",
      "Epoch 8/30 \t loss=0.5548 \t val_loss=0.5931 \t time=35.20s\n",
      "Epoch 9/30 \t loss=0.5467 \t val_loss=0.5862 \t time=35.22s\n",
      "Epoch 10/30 \t loss=0.5304 \t val_loss=0.5825 \t time=35.19s\n",
      "Epoch 11/30 \t loss=0.5097 \t val_loss=0.5778 \t time=35.25s\n",
      "Epoch 12/30 \t loss=0.5088 \t val_loss=0.5721 \t time=35.21s\n",
      "Epoch 13/30 \t loss=0.4897 \t val_loss=0.5704 \t time=35.25s\n",
      "Epoch 14/30 \t loss=0.4746 \t val_loss=0.5722 \t time=35.15s\n",
      "Epoch 15/30 \t loss=0.4638 \t val_loss=0.5764 \t time=35.12s\n",
      "Epoch 16/30 \t loss=0.4397 \t val_loss=0.5763 \t time=35.18s\n",
      "Epoch 17/30 \t loss=0.4320 \t val_loss=0.5919 \t time=35.15s\n",
      "Epoch 18/30 \t loss=0.4225 \t val_loss=0.5643 \t time=35.21s\n",
      "Epoch 19/30 \t loss=0.4052 \t val_loss=0.5709 \t time=35.20s\n",
      "Epoch 20/30 \t loss=0.3962 \t val_loss=0.5641 \t time=35.21s\n",
      "Epoch 21/30 \t loss=0.3910 \t val_loss=0.5653 \t time=35.14s\n",
      "Epoch 22/30 \t loss=0.3843 \t val_loss=0.5836 \t time=35.14s\n",
      "Epoch 23/30 \t loss=0.3672 \t val_loss=0.6060 \t time=35.12s\n",
      "Epoch 24/30 \t loss=0.3569 \t val_loss=0.5761 \t time=35.11s\n",
      "Epoch 25/30 \t loss=0.3522 \t val_loss=0.5846 \t time=35.15s\n",
      "Epoch 26/30 \t loss=0.3361 \t val_loss=0.5975 \t time=35.10s\n",
      "Epoch 27/30 \t loss=0.3422 \t val_loss=0.5766 \t time=35.18s\n",
      "Epoch 28/30 \t loss=0.3295 \t val_loss=0.6047 \t time=35.13s\n",
      "Epoch 29/30 \t loss=0.3013 \t val_loss=0.6030 \t time=35.25s\n",
      "Epoch 30/30 \t loss=0.3089 \t val_loss=0.5968 \t time=35.11s\n",
      "fold: 2\n",
      "Epoch 1/30 \t loss=0.9109 \t val_loss=0.8034 \t time=35.16s\n",
      "Epoch 2/30 \t loss=0.7629 \t val_loss=0.7097 \t time=35.19s\n",
      "Epoch 3/30 \t loss=0.6961 \t val_loss=0.6734 \t time=35.23s\n",
      "Epoch 4/30 \t loss=0.6388 \t val_loss=0.6491 \t time=35.19s\n",
      "Epoch 5/30 \t loss=0.6197 \t val_loss=0.6291 \t time=35.32s\n",
      "Epoch 6/30 \t loss=0.5999 \t val_loss=0.6234 \t time=35.20s\n",
      "Epoch 7/30 \t loss=0.5698 \t val_loss=0.6132 \t time=35.30s\n",
      "Epoch 8/30 \t loss=0.5545 \t val_loss=0.6064 \t time=35.16s\n",
      "Epoch 9/30 \t loss=0.5568 \t val_loss=0.6006 \t time=35.27s\n",
      "Epoch 10/30 \t loss=0.5329 \t val_loss=0.5959 \t time=35.21s\n",
      "Epoch 11/30 \t loss=0.5041 \t val_loss=0.5847 \t time=35.19s\n",
      "Epoch 12/30 \t loss=0.4913 \t val_loss=0.5703 \t time=35.27s\n",
      "Epoch 13/30 \t loss=0.4651 \t val_loss=0.5713 \t time=35.25s\n",
      "Epoch 14/30 \t loss=0.4665 \t val_loss=0.5640 \t time=35.18s\n",
      "Epoch 15/30 \t loss=0.4629 \t val_loss=0.5709 \t time=35.28s\n",
      "Epoch 16/30 \t loss=0.4221 \t val_loss=0.5598 \t time=35.18s\n",
      "Epoch 17/30 \t loss=0.4115 \t val_loss=0.5565 \t time=35.20s\n",
      "Epoch 18/30 \t loss=0.3990 \t val_loss=0.5648 \t time=35.24s\n",
      "Epoch 19/30 \t loss=0.3870 \t val_loss=0.5547 \t time=35.18s\n",
      "Epoch 20/30 \t loss=0.3909 \t val_loss=0.5625 \t time=35.22s\n",
      "Epoch 21/30 \t loss=0.3647 \t val_loss=0.5469 \t time=35.20s\n",
      "Epoch 22/30 \t loss=0.3643 \t val_loss=0.5527 \t time=35.20s\n",
      "Epoch 23/30 \t loss=0.3676 \t val_loss=0.5517 \t time=35.23s\n",
      "Epoch 24/30 \t loss=0.3313 \t val_loss=0.5489 \t time=35.20s\n",
      "Epoch 25/30 \t loss=0.3237 \t val_loss=0.5488 \t time=35.17s\n",
      "Epoch 26/30 \t loss=0.3275 \t val_loss=0.5457 \t time=35.13s\n",
      "Epoch 27/30 \t loss=0.3055 \t val_loss=0.5608 \t time=35.20s\n",
      "Epoch 28/30 \t loss=0.2959 \t val_loss=0.5596 \t time=35.20s\n",
      "Epoch 29/30 \t loss=0.2915 \t val_loss=0.5722 \t time=35.14s\n",
      "Epoch 30/30 \t loss=0.2984 \t val_loss=0.5664 \t time=35.21s\n",
      "fold: 3\n",
      "Epoch 1/30 \t loss=0.9143 \t val_loss=0.7875 \t time=35.23s\n",
      "Epoch 2/30 \t loss=0.7782 \t val_loss=0.6854 \t time=35.26s\n",
      "Epoch 3/30 \t loss=0.7035 \t val_loss=0.6456 \t time=35.23s\n",
      "Epoch 4/30 \t loss=0.6486 \t val_loss=0.6135 \t time=35.20s\n",
      "Epoch 5/30 \t loss=0.6310 \t val_loss=0.6146 \t time=35.23s\n",
      "Epoch 6/30 \t loss=0.6000 \t val_loss=0.5988 \t time=35.19s\n",
      "Epoch 7/30 \t loss=0.5738 \t val_loss=0.5788 \t time=35.23s\n",
      "Epoch 8/30 \t loss=0.5696 \t val_loss=0.5663 \t time=35.27s\n",
      "Epoch 9/30 \t loss=0.5428 \t val_loss=0.5643 \t time=35.23s\n",
      "Epoch 10/30 \t loss=0.5166 \t val_loss=0.5735 \t time=35.31s\n",
      "Epoch 11/30 \t loss=0.5112 \t val_loss=0.5595 \t time=35.17s\n",
      "Epoch 12/30 \t loss=0.4907 \t val_loss=0.5549 \t time=35.25s\n",
      "Epoch 13/30 \t loss=0.4758 \t val_loss=0.5508 \t time=35.24s\n",
      "Epoch 14/30 \t loss=0.4667 \t val_loss=0.5540 \t time=35.22s\n",
      "Epoch 15/30 \t loss=0.4452 \t val_loss=0.5428 \t time=35.19s\n",
      "Epoch 16/30 \t loss=0.4454 \t val_loss=0.5500 \t time=35.20s\n",
      "Epoch 17/30 \t loss=0.4328 \t val_loss=0.5542 \t time=35.22s\n",
      "Epoch 18/30 \t loss=0.4217 \t val_loss=0.5442 \t time=35.19s\n",
      "Epoch 19/30 \t loss=0.4091 \t val_loss=0.5589 \t time=35.24s\n",
      "Epoch 20/30 \t loss=0.3794 \t val_loss=0.5510 \t time=35.18s\n",
      "Epoch 21/30 \t loss=0.3739 \t val_loss=0.5534 \t time=35.21s\n",
      "Epoch 22/30 \t loss=0.3674 \t val_loss=0.5358 \t time=35.21s\n",
      "Epoch 23/30 \t loss=0.3687 \t val_loss=0.5339 \t time=35.20s\n",
      "Epoch 24/30 \t loss=0.3418 \t val_loss=0.5555 \t time=35.26s\n",
      "Epoch 25/30 \t loss=0.3451 \t val_loss=0.5377 \t time=35.17s\n",
      "Epoch 26/30 \t loss=0.3270 \t val_loss=0.5478 \t time=35.22s\n",
      "Epoch 27/30 \t loss=0.3283 \t val_loss=0.5411 \t time=35.22s\n",
      "Epoch 28/30 \t loss=0.3258 \t val_loss=0.5592 \t time=35.27s\n",
      "Epoch 29/30 \t loss=0.3028 \t val_loss=0.5799 \t time=35.18s\n",
      "Epoch 30/30 \t loss=0.2984 \t val_loss=0.5669 \t time=35.19s\n",
      "fold: 4\n",
      "Epoch 1/30 \t loss=0.9109 \t val_loss=0.8080 \t time=35.21s\n",
      "Epoch 2/30 \t loss=0.7594 \t val_loss=0.7219 \t time=35.25s\n",
      "Epoch 3/30 \t loss=0.6960 \t val_loss=0.6870 \t time=35.24s\n",
      "Epoch 4/30 \t loss=0.6482 \t val_loss=0.6593 \t time=35.31s\n",
      "Epoch 5/30 \t loss=0.6272 \t val_loss=0.6419 \t time=35.26s\n",
      "Epoch 6/30 \t loss=0.5954 \t val_loss=0.6369 \t time=35.29s\n",
      "Epoch 7/30 \t loss=0.5767 \t val_loss=0.6261 \t time=35.24s\n",
      "Epoch 8/30 \t loss=0.5612 \t val_loss=0.6298 \t time=35.29s\n",
      "Epoch 9/30 \t loss=0.5354 \t val_loss=0.6050 \t time=35.19s\n",
      "Epoch 10/30 \t loss=0.5188 \t val_loss=0.6044 \t time=35.25s\n",
      "Epoch 11/30 \t loss=0.5065 \t val_loss=0.6029 \t time=35.26s\n",
      "Epoch 12/30 \t loss=0.4996 \t val_loss=0.5868 \t time=35.23s\n",
      "Epoch 13/30 \t loss=0.4759 \t val_loss=0.5986 \t time=35.28s\n",
      "Epoch 14/30 \t loss=0.4707 \t val_loss=0.5990 \t time=35.23s\n",
      "Epoch 15/30 \t loss=0.4561 \t val_loss=0.5758 \t time=35.21s\n",
      "Epoch 16/30 \t loss=0.4207 \t val_loss=0.5755 \t time=35.20s\n",
      "Epoch 17/30 \t loss=0.4300 \t val_loss=0.6089 \t time=35.21s\n",
      "Epoch 18/30 \t loss=0.4113 \t val_loss=0.5791 \t time=35.17s\n",
      "Epoch 19/30 \t loss=0.4036 \t val_loss=0.5673 \t time=35.16s\n",
      "Epoch 20/30 \t loss=0.3863 \t val_loss=0.5925 \t time=35.27s\n",
      "Epoch 21/30 \t loss=0.3773 \t val_loss=0.5832 \t time=35.19s\n",
      "Epoch 22/30 \t loss=0.3760 \t val_loss=0.5553 \t time=35.23s\n",
      "Epoch 23/30 \t loss=0.3621 \t val_loss=0.5714 \t time=35.30s\n",
      "Epoch 24/30 \t loss=0.3527 \t val_loss=0.5680 \t time=35.20s\n",
      "Epoch 25/30 \t loss=0.3510 \t val_loss=0.5523 \t time=35.37s\n",
      "Epoch 26/30 \t loss=0.3214 \t val_loss=0.5697 \t time=35.25s\n",
      "Epoch 27/30 \t loss=0.3101 \t val_loss=0.5729 \t time=35.22s\n",
      "Epoch 28/30 \t loss=0.3003 \t val_loss=0.5626 \t time=35.17s\n",
      "Epoch 29/30 \t loss=0.2853 \t val_loss=0.5992 \t time=35.20s\n",
      "Epoch 30/30 \t loss=0.2741 \t val_loss=0.5786 \t time=35.18s\n",
      "fold: 5\n",
      "Epoch 1/30 \t loss=0.9132 \t val_loss=0.8352 \t time=35.20s\n",
      "Epoch 2/30 \t loss=0.7745 \t val_loss=0.7396 \t time=35.24s\n",
      "Epoch 3/30 \t loss=0.7066 \t val_loss=0.6872 \t time=35.21s\n",
      "Epoch 4/30 \t loss=0.6316 \t val_loss=0.6667 \t time=35.24s\n",
      "Epoch 5/30 \t loss=0.6220 \t val_loss=0.6355 \t time=35.21s\n",
      "Epoch 6/30 \t loss=0.5712 \t val_loss=0.6245 \t time=35.20s\n",
      "Epoch 7/30 \t loss=0.5743 \t val_loss=0.5966 \t time=35.30s\n",
      "Epoch 8/30 \t loss=0.5402 \t val_loss=0.6029 \t time=35.24s\n",
      "Epoch 9/30 \t loss=0.5309 \t val_loss=0.5910 \t time=35.28s\n",
      "Epoch 10/30 \t loss=0.5004 \t val_loss=0.5876 \t time=35.24s\n",
      "Epoch 11/30 \t loss=0.5097 \t val_loss=0.5857 \t time=35.25s\n",
      "Epoch 12/30 \t loss=0.4826 \t val_loss=0.5670 \t time=35.19s\n",
      "Epoch 13/30 \t loss=0.4781 \t val_loss=0.5695 \t time=35.21s\n",
      "Epoch 14/30 \t loss=0.4568 \t val_loss=0.5486 \t time=35.22s\n",
      "Epoch 15/30 \t loss=0.4366 \t val_loss=0.5557 \t time=35.26s\n",
      "Epoch 16/30 \t loss=0.4456 \t val_loss=0.5451 \t time=35.23s\n",
      "Epoch 17/30 \t loss=0.4343 \t val_loss=0.5678 \t time=35.22s\n",
      "Epoch 18/30 \t loss=0.4082 \t val_loss=0.5532 \t time=35.25s\n",
      "Epoch 19/30 \t loss=0.4000 \t val_loss=0.5523 \t time=35.15s\n",
      "Epoch 20/30 \t loss=0.3902 \t val_loss=0.5428 \t time=35.20s\n",
      "Epoch 21/30 \t loss=0.3746 \t val_loss=0.5495 \t time=35.25s\n",
      "Epoch 22/30 \t loss=0.3776 \t val_loss=0.5384 \t time=35.14s\n",
      "Epoch 23/30 \t loss=0.3521 \t val_loss=0.5473 \t time=35.21s\n",
      "Epoch 24/30 \t loss=0.3416 \t val_loss=0.5440 \t time=35.17s\n",
      "Epoch 25/30 \t loss=0.3498 \t val_loss=0.5484 \t time=35.21s\n",
      "Epoch 26/30 \t loss=0.3265 \t val_loss=0.5450 \t time=35.16s\n",
      "Epoch 27/30 \t loss=0.3142 \t val_loss=0.5420 \t time=35.27s\n",
      "Epoch 28/30 \t loss=0.3134 \t val_loss=0.5412 \t time=35.14s\n",
      "Epoch 29/30 \t loss=0.3099 \t val_loss=0.5489 \t time=35.16s\n",
      "Epoch 30/30 \t loss=0.3063 \t val_loss=0.5324 \t time=35.18s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    y = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    return y\n",
    "\n",
    "output = np.zeros((len(test_df), 3))\n",
    "testset = MyDataset(test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=20) #data loader for test dataset\n",
    "\n",
    "n_epochs = 30\n",
    "#Use Kfold to get robusted score\n",
    "for n_fold, (train_index, val_index) in enumerate(Kfold):\n",
    "    min_val_loss = 100.0 # for save best model\n",
    "    PATH = \"./best_model_{}.hdf5\".format(n_fold+1)\n",
    "    \n",
    "    train_df = train.loc[train_index]\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df   = train.loc[val_index]\n",
    "    val_df   = val_df.reset_index(drop=True)\n",
    "    \n",
    "    trainset = MyDataset(train_df)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=20, shuffle=True)\n",
    "    valset = MyDataset(val_df)\n",
    "    val_loader = torch.utils.data.DataLoader(valset, batch_size=20, shuffle=True)\n",
    "    \n",
    "    model = score_model()\n",
    "    #freeze bert\n",
    "    set_trainable(model.bert, False)\n",
    "    set_trainable(model.embedding, True) \n",
    "    set_trainable(model.pair_score, True)\n",
    "    model.cuda() #\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001) \n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    \n",
    "    print('fold:', n_fold+1)\n",
    "    for i in range(n_epochs):\n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        model.train() \n",
    "        avg_loss = 0.\n",
    "        for idx, (inputs, label) in enumerate(train_loader):\n",
    "            index_token, offset, distP_A, distP_B = inputs\n",
    "            index_token = index_token.type(torch.LongTensor).cuda() #change IntTensor to LongTensor,\n",
    "            offset      = offset.type(torch.LongTensor).cuda()\n",
    "            label       = label.type(torch.LongTensor).cuda()\n",
    "            distP_A     = distP_A.type(torch.LongTensor).cuda()\n",
    "            distP_B     = distP_B.type(torch.LongTensor).cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output_train = model(index_token, offset, distP_A, distP_B)\n",
    "            loss = criterion(output_train, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        avg_val_loss = 0.\n",
    "        #Start test\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (inputs, label) in enumerate(val_loader):\n",
    "                index_token, offset, distP_A, distP_B = inputs\n",
    "                index_token = index_token.type(torch.LongTensor).cuda()\n",
    "                offset      = offset.type(torch.LongTensor).cuda()\n",
    "                label       = label.type(torch.LongTensor).cuda()\n",
    "                distP_A     = distP_A.type(torch.LongTensor).cuda()\n",
    "                distP_B     = distP_B.type(torch.LongTensor).cuda()\n",
    "                \n",
    "                output_test =  model(index_token, offset, distP_A, distP_B)\n",
    "                avg_val_loss += criterion(output_test, label).item() / len(val_loader)\n",
    "                \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "                i + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "        # save best model\n",
    "        if min_val_loss > avg_val_loss:\n",
    "            min_val_loss = avg_val_loss \n",
    "            torch.save(model.state_dict(), PATH)\n",
    "        \n",
    "    \n",
    "    del model\n",
    "    \n",
    "    model = score_model()\n",
    "    model.load_state_dict(torch.load(PATH)) #load best model to predict\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, label) in enumerate(test_loader):\n",
    "            index_token, offset, distP_A, distP_B = inputs\n",
    "            index_token = index_token.type(torch.LongTensor).cuda()\n",
    "            offset      = offset.type(torch.LongTensor).cuda()\n",
    "            label       = label.type(torch.LongTensor).cuda()\n",
    "            distP_A     = distP_A.type(torch.LongTensor).cuda()\n",
    "            distP_B     = distP_B.type(torch.LongTensor).cuda()\n",
    "                \n",
    "            y_pred = model(index_token, offset, distP_A, distP_B)\n",
    "            y_pred = softmax(y_pred.cpu().numpy())\n",
    "            start = idx * 20\n",
    "            end = start + 20\n",
    "            output[start:end, :] += y_pred                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000075809a8e6b062f5fb3c191a8ed52</td>\n",
       "      <td>0.541942</td>\n",
       "      <td>0.432975</td>\n",
       "      <td>0.025083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005d0f3b0a6c9ffbd31a48453029911</td>\n",
       "      <td>0.994363</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.005477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007775c40bedd4147a0573d66dc28f8</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.966405</td>\n",
       "      <td>0.024465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001194e3fe1234d00198ef6bba4cc588</td>\n",
       "      <td>0.031153</td>\n",
       "      <td>0.381779</td>\n",
       "      <td>0.587069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014bb7085278ef3f9b74f14771caca9</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.999161</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>002671a4f3ec8d724e0541c2f1a1f8cc</td>\n",
       "      <td>0.987315</td>\n",
       "      <td>0.012328</td>\n",
       "      <td>0.000357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>002eb2ad27bbeab286b15650b8cb2c27</td>\n",
       "      <td>0.889726</td>\n",
       "      <td>0.045945</td>\n",
       "      <td>0.064329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>002eebd91abbd53207da15ae61714531</td>\n",
       "      <td>0.494550</td>\n",
       "      <td>0.462993</td>\n",
       "      <td>0.042458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0034d8a107da2eeba335128665f6fc1f</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.998401</td>\n",
       "      <td>0.001506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0034dfd350220409b621a5e3ac1c5e02</td>\n",
       "      <td>0.640080</td>\n",
       "      <td>0.298515</td>\n",
       "      <td>0.061405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0038594ac3031cfe28506b2356a164f0</td>\n",
       "      <td>0.050523</td>\n",
       "      <td>0.707170</td>\n",
       "      <td>0.242307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>003c6dedd37fd9e61fe7d4caeeb6cce2</td>\n",
       "      <td>0.960733</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.033494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>003d42668ef51e38ef23747e0e63ae2a</td>\n",
       "      <td>0.927501</td>\n",
       "      <td>0.066158</td>\n",
       "      <td>0.006340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>004d34d0538140b06c9fb8a5a79be949</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.119155</td>\n",
       "      <td>0.065176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>004e46b35db66402fa8046fd7bd800fb</td>\n",
       "      <td>0.304951</td>\n",
       "      <td>0.657427</td>\n",
       "      <td>0.037622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0053b6a829b09c988164aaec0ee2fb4b</td>\n",
       "      <td>0.041356</td>\n",
       "      <td>0.842913</td>\n",
       "      <td>0.115732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0056e846c6c6a948326fac4caebac70d</td>\n",
       "      <td>0.545896</td>\n",
       "      <td>0.274505</td>\n",
       "      <td>0.179599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00587d0e442b7b4521f685d12f8b78d3</td>\n",
       "      <td>0.220507</td>\n",
       "      <td>0.456451</td>\n",
       "      <td>0.323043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>006506c9485f14a3d850b2c7dc06d28c</td>\n",
       "      <td>0.198422</td>\n",
       "      <td>0.700053</td>\n",
       "      <td>0.101525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00666e20548dc00c0506444b39522d1c</td>\n",
       "      <td>0.062866</td>\n",
       "      <td>0.650273</td>\n",
       "      <td>0.286860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID         A         B   NEITHER\n",
       "0   000075809a8e6b062f5fb3c191a8ed52  0.541942  0.432975  0.025083\n",
       "1   0005d0f3b0a6c9ffbd31a48453029911  0.994363  0.000160  0.005477\n",
       "2   0007775c40bedd4147a0573d66dc28f8  0.009130  0.966405  0.024465\n",
       "3   001194e3fe1234d00198ef6bba4cc588  0.031153  0.381779  0.587069\n",
       "4   0014bb7085278ef3f9b74f14771caca9  0.000540  0.999161  0.000299\n",
       "5   002671a4f3ec8d724e0541c2f1a1f8cc  0.987315  0.012328  0.000357\n",
       "6   002eb2ad27bbeab286b15650b8cb2c27  0.889726  0.045945  0.064329\n",
       "7   002eebd91abbd53207da15ae61714531  0.494550  0.462993  0.042458\n",
       "8   0034d8a107da2eeba335128665f6fc1f  0.000093  0.998401  0.001506\n",
       "9   0034dfd350220409b621a5e3ac1c5e02  0.640080  0.298515  0.061405\n",
       "10  0038594ac3031cfe28506b2356a164f0  0.050523  0.707170  0.242307\n",
       "11  003c6dedd37fd9e61fe7d4caeeb6cce2  0.960733  0.005773  0.033494\n",
       "12  003d42668ef51e38ef23747e0e63ae2a  0.927501  0.066158  0.006340\n",
       "13  004d34d0538140b06c9fb8a5a79be949  0.815668  0.119155  0.065176\n",
       "14  004e46b35db66402fa8046fd7bd800fb  0.304951  0.657427  0.037622\n",
       "15  0053b6a829b09c988164aaec0ee2fb4b  0.041356  0.842913  0.115732\n",
       "16  0056e846c6c6a948326fac4caebac70d  0.545896  0.274505  0.179599\n",
       "17  00587d0e442b7b4521f685d12f8b78d3  0.220507  0.456451  0.323043\n",
       "18  006506c9485f14a3d850b2c7dc06d28c  0.198422  0.700053  0.101525\n",
       "19  00666e20548dc00c0506444b39522d1c  0.062866  0.650273  0.286860"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "output /= 5 \n",
    "sub_df_path = os.path.join('../input/gendered-pronoun-resolution/', 'sample_submission_stage_2.csv')\n",
    "sub_df = pd.read_csv(sub_df_path)\n",
    "sub_df.loc[:, 'A'] = pd.Series(output[:, 0])\n",
    "sub_df.loc[:, 'B'] = pd.Series(output[:, 1])\n",
    "sub_df.loc[:, 'NEITHER'] = pd.Series(output[:, 2])\n",
    "\n",
    "sub_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48540048344531606"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.read_csv('../working/test.csv')['label']\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "y_one_hot = np.zeros((2000, 3))\n",
    "for i in range(len(y_test)):\n",
    "    y_one_hot[i, y_test[i]] = 1\n",
    "log_loss(y_one_hot, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7995\n"
     ]
    }
   ],
   "source": [
    "_output = np.argmax(output, axis=1)\n",
    "print('acc:', np.asarray(np.where(_output == y_test)).shape[1]/ 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
